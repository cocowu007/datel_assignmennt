{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc8d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important notes:\n",
    "# In order to run the notebook and script successfully in a conda environment,\n",
    "# you need to install necessary dependencies:\n",
    "# torch numpy matplotlib import_ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f34ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4d41c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Dataset Class:\n",
    "# Use PyTorch's Dataset class to encapsulate satellite image data and masks into datasets \n",
    "# that can be used for model training\n",
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, data_list, transform=None):\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get orginal image and mask\n",
    "        image = self.data_list[idx]['image']\n",
    "        mask = self.data_list[idx]['mask']\n",
    "\n",
    "        #print(f\"Original image shape: {image.shape}\")  # Print original image shape\n",
    "        #print(f\"Original mask shape: {mask.shape}\")    # print original image shape\n",
    "        #print(f\"Original mask unique values: {np.unique(mask)}\")  # Print unique number in mask\n",
    "\n",
    "        # Normalization\n",
    "        image = image.astype(np.float32) / 10000\n",
    "        mask = mask.astype(np.float32)\n",
    "\n",
    "        # Check the unique number in mask after normalization\n",
    "        #print(f\"Normalized mask unique values: {np.unique(mask)}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert to tensor\n",
    "        image = torch.from_numpy(image.transpose(2, 0, 1))  # HWC -> CHW\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        #print(f\"Transformed image shape: {image.shape}\")  \n",
    "        #print(f\"Transformed mask shape: {mask.shape}\")    \n",
    "        #print(f\"tensor mask unique values: {np.unique(mask)}\")\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define the model:\n",
    "# Use a pre-trained UNet model, for brevity, define a simple UNet model manually here\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),  # Dropout after ReLU\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),  # Dropout after ReLU\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),  # Dropout after ReLU\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Define loss function - DiceLoss:\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        intersection = (outputs * targets).sum(dim=[2, 3])  # Sum over spatial dimensions\n",
    "        union = outputs.sum(dim=[2, 3]) + targets.sum(dim=[2, 3])\n",
    "\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        dice_loss = 1 - dice.mean()\n",
    "        return dice_loss\n",
    "\n",
    "# Define loss function - FocalLoss:\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        #BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')  # Using logits for numerical stability\n",
    "        BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none') # Using numerical stability\n",
    "        pt = torch.exp(-BCE_loss)  # Probability of true class\n",
    "        Focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return Focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return Focal_loss.sum()\n",
    "        else:\n",
    "            return Focal_loss\n",
    "\n",
    "# Define loss function - CombinedLoss:\n",
    "# DiceLoss + FocalLoss\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0, dice_weight=3.0, focal_weight=1.0, alpha=1.0, gamma=2.0):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice_loss = DiceLoss(smooth=smooth)\n",
    "        self.focal_loss = FocalLoss(alpha=alpha, gamma=gamma)\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_weight = focal_weight\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        dice = self.dice_loss(outputs, targets)\n",
    "        focal = self.focal_loss(outputs, targets)\n",
    "        return self.dice_weight * dice + self.focal_weight * focal\n",
    "\n",
    "# Define Training Functions:\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, masks in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            #print(f\"train_model - Model output min: {outputs.min()}, max: {outputs.max()}\")\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Define Evaluation Functions:\n",
    "# Use IoU, Dice Coefficient, Precision, Recall, F1 Score as the evaluation metric\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    thresholds = [0.1, 0.3, 0.5, 0.6, 0.7]\n",
    "    for threshold in thresholds:\n",
    "        total_iou = 0.0\n",
    "        total_dice = 0.0\n",
    "        total_precision = 0.0\n",
    "        total_recall = 0.0\n",
    "        total_f1 = 0.0\n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "        for inputs, masks in dataloader:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "                predicted_masks = (outputs > threshold).float()\n",
    "            \n",
    "                # Print some example predictions and masks for debugging\n",
    "                #print(f\"evaluate_model- Model output min: {outputs.min()}, max: {outputs.max()}\")\n",
    "                #print(f\"evaluate_model - predicted_masks min: {predicted_masks.min()}, max: {predicted_masks.max()}\")\n",
    "            \n",
    "                intersection = (predicted_masks * masks).sum().float()\n",
    "                union = (predicted_masks.sum() + masks.sum() - intersection).float()\n",
    "\n",
    "                # Calculate IoU\n",
    "                iou = intersection / union if union != 0 else torch.tensor(0.0)\n",
    "                total_iou += iou.item()\n",
    "\n",
    "                # Calculate Dice Coefficient\n",
    "                dice = (2 * intersection) / (predicted_masks.sum() + masks.sum()) if (predicted_masks.sum() + masks.sum()) != 0 else torch.tensor(0.0)\n",
    "                total_dice += dice.item()\n",
    "    \n",
    "                # Calculate Precision\n",
    "                tp = (predicted_masks * masks).sum().float()  # True Positives\n",
    "                fp = (predicted_masks * (1 - masks)).sum().float()  # False Positives\n",
    "                precision = tp / (tp + fp) if (tp + fp) != 0 else torch.tensor(0.0)\n",
    "                total_precision += precision.item()\n",
    "\n",
    "                # Calculate Recall\n",
    "                fn = ((1 - predicted_masks) * masks).sum().float()  # False Negatives\n",
    "                recall = tp / (tp + fn) if (tp + fn) != 0 else torch.tensor(0.0)\n",
    "                total_recall += recall.item()\n",
    "\n",
    "                # Calculate F1 Score\n",
    "                f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else torch.tensor(0.0)\n",
    "                total_f1 += f1.item()\n",
    "                \n",
    "        print(f\"Validation threshold: {threshold}\")\n",
    "        print(f\"Validation IoU: {total_iou / num_batches:.4f}\")\n",
    "        print(f\"Validation Dice Coefficient: {total_dice / num_batches:.4f}\")\n",
    "        print(f\"Validation Precision: {total_precision / num_batches:.4f}\")\n",
    "        print(f\"Validation Recall: {total_recall / num_batches:.4f}\")\n",
    "        print(f\"Validation F1 Score: {total_f1 / num_batches:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5d5cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for running inference and calculating IoU on validation set\n",
    "def run_inference(model_file, dataset_file):\n",
    "    with open(dataset_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    val_dataset = SatelliteDataset(data['val'])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "    # Load the model\n",
    "    model = UNet()\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    # Run inference and evaluate\n",
    "    evaluate_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc345613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 3.8161\n",
      "Epoch [2/5], Loss: 3.7946\n",
      "Epoch [3/5], Loss: 3.7503\n",
      "Epoch [4/5], Loss: 3.7121\n",
      "Epoch [5/5], Loss: 3.6646\n",
      "Validation threshold: 0.1\n",
      "Validation IoU: 0.0709\n",
      "Validation Dice Coefficient: 0.1299\n",
      "Validation Precision: 0.0710\n",
      "Validation Recall: 0.9728\n",
      "Validation F1 Score: 0.1299\n",
      "Validation threshold: 0.3\n",
      "Validation IoU: 0.1124\n",
      "Validation Dice Coefficient: 0.1964\n",
      "Validation Precision: 0.1142\n",
      "Validation Recall: 0.8684\n",
      "Validation F1 Score: 0.1964\n",
      "Validation threshold: 0.5\n",
      "Validation IoU: 0.1336\n",
      "Validation Dice Coefficient: 0.2275\n",
      "Validation Precision: 0.1515\n",
      "Validation Recall: 0.5372\n",
      "Validation F1 Score: 0.2275\n",
      "Validation threshold: 0.6\n",
      "Validation IoU: 0.0144\n",
      "Validation Dice Coefficient: 0.0283\n",
      "Validation Precision: 0.1461\n",
      "Validation Recall: 0.0171\n",
      "Validation F1 Score: 0.0283\n",
      "Validation threshold: 0.7\n",
      "Validation IoU: 0.0000\n",
      "Validation Dice Coefficient: 0.0001\n",
      "Validation Precision: 0.1605\n",
      "Validation Recall: 0.0000\n",
      "Validation F1 Score: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# In order to avoid running the rest of the code of the entire notebook when calling run_inference,\n",
    "# separate the execution logic from the function definition by using if __name__ == ‘__main__’ block\n",
    "\n",
    "# This way, operations such as training, validation, and model saving will only be performed when the notebook \n",
    "# is running directly, and will not be triggered when someone just imports and calls run_inference\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset.pickle file\n",
    "    with open('dataset.pickle', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # Dataset format: [{\"image\": np.array, \"mask\": np.array}]\n",
    "    train_data = data['train']\n",
    "    val_data = data['val']\n",
    "\n",
    "    # Create train and validation datasets\n",
    "    train_dataset = SatelliteDataset(train_data)\n",
    "    val_dataset = SatelliteDataset(val_data)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "    # Instantiate the model, loss function, and optimizer\n",
    "    model = UNet(dropout_prob=0.3)\n",
    "    # criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    # criterion = DiceLoss(smooth=1.0)\n",
    "    criterion = CombinedLoss(smooth=1.0, dice_weight=4.0, focal_weight=0.5, alpha=1.0, gamma=2.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs=5)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(model, val_loader)\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), \"unet_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932280d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda base)",
   "language": "python",
   "name": "conda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
